{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "lvu9i-NYbOmR",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lvu9i-NYbOmR",
        "outputId": "876cfdba-0ede-407b-cac8-69e416bceb49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "pVODr_KqTOHi",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVODr_KqTOHi",
        "outputId": "2e8962e0-df7b-483c-b371-b5442ec1b201"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tokenizers in /usr/local/lib/python3.9/dist-packages (0.13.3)\n"
          ]
        }
      ],
      "source": [
        "!pip install tokenizers\n",
        "from tokenizers.normalizers import NFD\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "9a957e06-a7c1-420f-a588-2dfa79c979d0",
      "metadata": {
        "id": "9a957e06-a7c1-420f-a588-2dfa79c979d0"
      },
      "outputs": [],
      "source": [
        "## TORCH IMPORTS\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "from torch import nn, Tensor,multinomial\n",
        "import torch.nn.functional as F\n",
        "from torch.nn import TransformerEncoder, TransformerEncoderLayer, Transformer\n",
        "from torch.utils.data import dataset, DataLoader\n",
        "from torch.autograd import Variable\n",
        "from torchtext.vocab import Vectors, vocab,build_vocab_from_iterator,vocab\n",
        "\n",
        "## OTHER IMPORTS\n",
        "import math\n",
        "import re\n",
        "import pickle\n",
        "from collections import defaultdict, OrderedDict, Counter\n",
        "from tqdm import tqdm\n",
        "import numpy as np\n",
        "from numpy.random import uniform"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "zaV1SMr9LG18",
      "metadata": {
        "id": "zaV1SMr9LG18"
      },
      "outputs": [],
      "source": [
        "## CONSTANTS\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "langs_codes = {\"Gitksan\": \"git\", \"Arapaho\":\"arp\", \"Lezgi\":\"lez\", \"Nyangbo\":\"nyb\", \"Tsez\":\"ddo\",\"Uspanteko\":\"usp\"}\n",
        "START = \"<start>\"\n",
        "END = \"<end>\"\n",
        "UNK = \"<unk>\"\n",
        "PAD = \"<pad>\"\n",
        "BD = \"<bd>\"\n",
        "SP = \"#\"\n",
        "SPECIALS = [PAD, START, END,UNK,SP]\n",
        "normalizer = NFD()\n",
        "BATCH_SIZE = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "0d1f0d13-c1d2-43c8-8024-9b844a87ce4e",
      "metadata": {
        "id": "0d1f0d13-c1d2-43c8-8024-9b844a87ce4e"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def get_segmented_data(ld,splits=[\"transcriptions\",\"glosses\"]):\n",
        "    def get_segmented(line):\n",
        "        allsplits=[]\n",
        "\n",
        "        splits=[m.split() for m in line]\n",
        "        for i in splits:\n",
        "            for m in i:\n",
        "                allsplits.append(m)\n",
        "        return allsplits\n",
        "\n",
        "    data = {}\n",
        "    for split in splits:\n",
        "        data[split] = get_segmented(ld[split])\n",
        "    return data\n",
        "    \n",
        "def tag_bd(seq):\n",
        "\n",
        "  for i, c in enumerate(seq):\n",
        "    if i < len(seq)-2:\n",
        "      if seq[i+1] == \"-\":\n",
        "          seq[i] += seq[i+1]\n",
        "          seq[i+2] = seq[i+1] + seq[i+2]\n",
        "  return [i for i in seq if not i ==\"-\"]\n",
        "\n",
        "orth_process = lambda x: [START] + list(re.sub(\" \", \"#\", x)) + [END]\n",
        "gloss_process = lambda x: [START] + tag_bd(list(re.sub(\" \", \"#\", x))) + [END]\n",
        "\n",
        "\n",
        "def get_linesplits(orths,glosses):\n",
        "  ex=[]\n",
        "  for o,g in zip(orths, glosses):\n",
        "    orth = orth_process(o)\n",
        "    gloss = gloss_process(g)\n",
        "    ex.append((orth,gloss))\n",
        "   \n",
        "  return ex\n",
        "\n",
        "\n",
        "def get_train_val_split(data, props=10):\n",
        "    test_data = []\n",
        "    train_data = []\n",
        "    for i, dat in enumerate(data):\n",
        "        if i%props == 0:\n",
        "            test_data.append(dat)\n",
        "        else:\n",
        "            train_data.append(dat)\n",
        "    return test_data, train_data\n",
        "\n",
        "\n",
        "def get_windows(glosses,prop=0.5):\n",
        "    counts=defaultdict(int)\n",
        "    for gloss in glosses:\n",
        "        counts[len(gloss.split())]+=1\n",
        "    \n",
        "    sorted_counts = sorted(counts.items(),key=lambda x:x[1],reverse=True)\n",
        "    # get upper limit by finding average, then the max len of the lower prop% below average\n",
        "    lcounts = [l for l,c in sorted_counts]\n",
        "    avg = round(sum(lcounts)/len(lcounts))*prop\n",
        "    \n",
        "    while True:\n",
        "        windows=np.random.randint(1,avg,int(avg-2))\n",
        "        if len(set(windows)) == int(avg-2):\n",
        "            break\n",
        "            \n",
        "    return windows\n",
        "\n",
        "def get_window_segs(transcriptions,glosses, windows):\n",
        "    \n",
        "    window_segs = []\n",
        "    for trans,gls in zip(transcriptions,glosses):\n",
        "        transsplit=trans.split()\n",
        "        glssplit=gls.split()\n",
        "        for window in windows:\n",
        "            if len(glssplit) < window:\n",
        "                continue\n",
        "            sid = 0\n",
        "            for i in range(len(glssplit)-1):\n",
        "                transwind = transsplit[sid:sid+window]\n",
        "                glsswind = glssplit[sid:sid+window]\n",
        "                \n",
        "                if len(transwind) < window:\n",
        "                    continue\n",
        "                window_segs.append((orth_process(\" \".join(transwind)),gloss_process(\" \".join(glsswind))))\n",
        "\n",
        "                sid+=1\n",
        "    return window_segs\n",
        "    \n",
        "\n",
        "class LangIGT:\n",
        "    def __init__(self, lang):\n",
        "      \n",
        "        self.lang = lang\n",
        "        self.batch_size = BATCH_SIZE\n",
        "        \n",
        "        self.train_ld, self.dev_ld = self.get_langdict()\n",
        "        self.windows = get_windows(self.train_ld[\"glosses\"])\n",
        "\n",
        "        self.train_splits, self.dev_splits = get_linesplits(self.train_ld[\"transcriptions\"],self.train_ld[\"glosses\"]), get_linesplits(self.dev_ld[\"transcriptions\"],self.dev_ld[\"glosses\"])\n",
        "        self.train_windows, self.dev_windows = get_window_segs(self.train_ld[\"transcriptions\"],self.train_ld[\"glosses\"], self.windows), get_window_segs(self.dev_ld[\"transcriptions\"],self.dev_ld[\"glosses\"],self.windows)\n",
        "        self.gls_alphabet, self.orth_alphabet = self.get_alphabet()\n",
        "        \n",
        "    \n",
        "    def get_langdict(self):\n",
        "        ld = {\n",
        "            \"train\": defaultdict(list),\n",
        "            \"dev\": defaultdict(list)\n",
        "        }\n",
        "        path = f\"/content/drive/MyDrive/2023glossingST/data/{self.lang}/\"\n",
        "        #path = f\"data/{self.lang}/\"\n",
        "        train_fn = f\"{langs_codes[self.lang]}-train-track2-uncovered\"\n",
        "        dev_fn = f\"{langs_codes[self.lang]}-dev-track2-uncovered\"\n",
        "\n",
        "        for fp in [train_fn, dev_fn]:\n",
        "            data_type = fp.split(\"-\")[1]\n",
        "            with open(path + fp, \"r\",encoding=\"utf-8\") as f:\n",
        "                for line in f.readlines():\n",
        "\n",
        "                    if line.startswith(\"\\\\t\"):\n",
        "                        ld[data_type][\"transcriptions\"].append(line.lstrip(\"\\\\t \").rstrip(\"\\n\"))\n",
        "                    if line.startswith(\"\\g\"):\n",
        "                        ld[data_type][\"glosses\"].append(line.lstrip(\"\\\\g \").rstrip(\"\\n\"))\n",
        "                    if line.startswith(\"\\l\"):\n",
        "                        ld[data_type][\"translation\"].append(line.lstrip(\"\\\\l \").rstrip(\"\\n\"))\n",
        "\n",
        "        return ld[\"train\"],ld[\"dev\"]\n",
        "\n",
        "\n",
        "    def get_alphabet(self):\n",
        "      gloss_counter, orth_counter = Counter(), Counter()\n",
        "      for gloss,trans in zip(self.train_ld[\"glosses\"],self.train_ld[\"transcriptions\"]):\n",
        "        for g,t in zip(tag_bd(list(re.sub(\" \", \"#\", gloss))), list(re.sub(\" \", \"#\", trans))):\n",
        "          gloss_counter.update([g])\n",
        "          orth_counter.update([t])\n",
        "\n",
        "      gloss_voc = vocab(OrderedDict(sorted(gloss_counter.items(), key=lambda x: x[1], reverse=True)), specials=SPECIALS)\n",
        "      gloss_voc.set_default_index(gloss_voc[UNK])\n",
        "      orth_voc = vocab(OrderedDict(sorted(orth_counter.items(), key=lambda x: x[1], reverse=True)), specials=SPECIALS)\n",
        "      orth_voc.set_default_index(orth_voc[UNK])\n",
        "      return gloss_voc, orth_voc\n",
        "\n",
        "    def data(self,split):\n",
        "      train, test = [], []\n",
        "\n",
        "      for i in self.train_splits:\n",
        "        train.append(i)\n",
        "      for j in self.train_windows:\n",
        "        train.append(j)\n",
        "\n",
        "      for k in self.dev_splits:\n",
        "        test.append(k)\n",
        "      for l in self.dev_windows:\n",
        "        test.append(l)\n",
        "\n",
        "      dev, train = get_train_val_split(train)\n",
        "      \n",
        "      \n",
        "\n",
        "      data = {\n",
        "            \"train\" : train,\n",
        "            \"dev\" : dev,\n",
        "            \"test\" : test\n",
        "        }\n",
        "      return data[split]\n",
        "\n",
        "    def alphabet(self,split):\n",
        "      alphabet = {\n",
        "            \"gloss\" : self.gls_alphabet,\n",
        "            \"orth\" : self.orth_alphabet\n",
        "        }\n",
        "      return alphabet[split]\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "GsGiHwomh4i8",
      "metadata": {
        "id": "GsGiHwomh4i8"
      },
      "outputs": [],
      "source": [
        "lang = LangIGT(\"Lezgi\")\n",
        "train,dev, test = lang.data(\"train\"), lang.data(\"dev\"), lang.data(\"test\")\n",
        "gloss_alpha, orth_alpha = lang.alphabet(\"gloss\"), lang.alphabet(\"orth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "00tend8MUX13",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00tend8MUX13",
        "outputId": "d7801dd4-c242-4b44-db76-3e540660f0a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "window sizes: [8 6 7 4 5 3 1]\n",
            "size of training set: 28308\n",
            "size of dev set: 3146\n",
            "size of test set: 4705\n"
          ]
        }
      ],
      "source": [
        "print(f\"window sizes: {lang.windows}\")\n",
        "print(f\"size of training set: {len(train)}\")\n",
        "print(f\"size of dev set: {len(dev)}\")\n",
        "print(f\"size of test set: {len(test)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "a44851c7-8373-449f-a2e8-7ee6ba05da27",
      "metadata": {
        "id": "a44851c7-8373-449f-a2e8-7ee6ba05da27"
      },
      "outputs": [],
      "source": [
        "def tensor_transform(data):\n",
        "  orths=[]\n",
        "  for orth, _ in data:\n",
        "    input_tensor = torch.tensor([orth_alpha[i] for i in orth], dtype=torch.long)\n",
        "    orths.append(input_tensor)\n",
        "  return orths\n",
        "  \n",
        "def collate_batch(batch):\n",
        "    \n",
        "    input, output=[], []\n",
        "    for morphemes, glosses in batch:\n",
        "        input_tensor = torch.tensor([orth_alpha[i] for i in morphemes], dtype=torch.long)\n",
        "        output_tensor = torch.tensor([gloss_alpha[i] for i in glosses], dtype=torch.long)\n",
        "        input.append(input_tensor)\n",
        "        output.append(output_tensor)\n",
        "        # input_lengths.append(input_tensor.shape[0])\n",
        "        # output_lengths.append(output_tensor.shape[0])\n",
        "\n",
        "\n",
        "    return pad_sequence(input, \n",
        "                        batch_first=False, \n",
        "                        padding_value=orth_alpha[\"<pad>\"]), pad_sequence(output, \n",
        "                        batch_first=False, \n",
        "                        padding_value=gloss_alpha[\"<pad>\"])\n",
        "\n",
        "# Example((pad_sequence(input, \n",
        "#                                  batch_first=False, \n",
        "#                                  padding_value=src_alphabet[\"<pad>\"]), torch.tensor(input_lengths, dtype=torch.long)),\n",
        "#                        (pad_sequence(output, \n",
        "#                                  batch_first=False, \n",
        "#                                  padding_value=trg_alphabet[\"<pad>\"]), torch.tensor(output_lengths, dtype=torch.long)))\n",
        "\n",
        "\n",
        "def torchify(train,dev):\n",
        "    path = \"/content/drive/MyDrive/2023glossingST/\"\n",
        "    \n",
        "    train_dataloader = DataLoader(\n",
        "        train,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        collate_fn=collate_batch,\n",
        "        shuffle=True\n",
        "    )\n",
        "\n",
        "    dev_dataloader = DataLoader(\n",
        "        dev,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        collate_fn=collate_batch,\n",
        "        shuffle=False\n",
        "    )\n",
        "\n",
        "    test_dataloader = DataLoader(\n",
        "        test,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        collate_fn=collate_batch,\n",
        "        shuffle=False\n",
        "    )\n",
        "    return train_dataloader, dev_dataloader, test_dataloader\n",
        "\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "2l2bYuqHGdNZ",
      "metadata": {
        "id": "2l2bYuqHGdNZ"
      },
      "outputs": [],
      "source": [
        "train_dataloader, dev_dataloader, test_dataloader = torchify(train,dev)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "b4hhcxQda5wL",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b4hhcxQda5wL",
        "outputId": "9044cc04-7ade-4e56-a223-bb5455287f0f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor size of source language: torch.Size([54, 16])\n",
            "tensor size of target language: torch.Size([78, 16])\n",
            "the tensor of first example in target language: tensor([ 1, 37,  8, 17, 12,  6, 26, 53, 23, 21, 68,  4, 51, 28, 14,  6, 13,  4,\n",
            "        17, 30, 13, 53, 52, 16, 39,  4,  7,  6, 45, 32, 23, 16, 39,  2,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0])\n",
            "the tensor of first example in src language: tensor([ 1, 17,  8,  7,  7,  6,  4, 38, 24, 12,  5,  7,  4, 20, 34, 13,  7, 14,\n",
            "         7,  6,  4, 16, 22,  5, 25,  8,  7,  5,  2,  0,  0,  0,  0,  0,  0,  0,\n",
            "         0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0])\n",
            "['<start>', 'з', 'у', 'н', 'н', 'и', '#', 'И', 'м', 'р', 'а', 'н', '#', 'т', 'ф', 'е', 'н', 'г', 'н', 'и', '#', 'к', 'ъ', 'а', 'ч', 'у', 'н', 'а', '<end>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n",
            "['<start>', '1', 's', 'g', '.', 'a', 'b', 's-', '-A', 'N', 'D', '#', 'I', 'm', 'r', 'a', 'n', '#', 'g', 'u', 'n', 's-', '-F', 'O', 'C', '#', 't', 'a', 'k', 'e-', '-A', 'O', 'C', '<end>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>', '<pad>']\n"
          ]
        }
      ],
      "source": [
        "for batch in train_dataloader:\n",
        "    src, trg = batch\n",
        "    print('tensor size of source language:', src.shape)\n",
        "    print('tensor size of target language:', trg.shape)\n",
        "    print('the tensor of first example in target language:', trg[:, 0])\n",
        "    print('the tensor of first example in src language:', src[:, 0])\n",
        "    print([orth_alpha.get_itos()[i] for i in src[:, 0]])\n",
        "    print([gloss_alpha.get_itos()[i] for i in trg[:, 0]])\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "458e3721-f928-4816-93ba-c12cefef4c72",
      "metadata": {
        "id": "458e3721-f928-4816-93ba-c12cefef4c72"
      },
      "outputs": [],
      "source": [
        "\n",
        "# helper Module that adds positional encoding to the token embedding to introduce a notion of word order.\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self,\n",
        "                 emb_size: int,\n",
        "                 dropout: float,\n",
        "                 maxlen: int = max(len(i) for i in [g for _,g in train])+5):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        den = torch.exp(- torch.arange(0, emb_size, 2)* math.log(10000) / emb_size)\n",
        "        pos = torch.arange(0, maxlen).reshape(maxlen, 1)\n",
        "        pos_embedding = torch.zeros((maxlen, emb_size))\n",
        "        pos_embedding[:, 0::2] = torch.sin(pos * den)\n",
        "        pos_embedding[:, 1::2] = torch.cos(pos * den)\n",
        "        pos_embedding = pos_embedding.unsqueeze(-2)\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.register_buffer('pos_embedding', pos_embedding)\n",
        "\n",
        "    def forward(self, token_embedding: Tensor):\n",
        "        return self.dropout(token_embedding + self.pos_embedding[:token_embedding.size(0), :])\n",
        "\n",
        "# helper Module to convert tensor of input indices into corresponding tensor of token embeddings\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, emb_size, embedding: Vectors):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        #self.embedding = nn.Embedding(vocab_size, emb_size)\n",
        "        self.embed = embedding\n",
        "        #self.emb_size = emb_size\n",
        "        self.emb_size = emb_size\n",
        "\n",
        "    def forward(self, tokens: Tensor):\n",
        "        return self.embed(tokens.long()) * math.sqrt(self.emb_size)\n",
        "\n",
        "# Seq2Seq Network\n",
        "class Seq2SeqTransformer(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_encoder_layers: int,\n",
        "                 num_decoder_layers: int,\n",
        "                 emb_size: int,\n",
        "                 nhead: int,\n",
        "                 src_vocab_size: int,\n",
        "                 tgt_vocab_size: int,\n",
        "                 src_embeddings: nn.Embedding,\n",
        "                 tgt_embeddings: Vectors,\n",
        "                 dim_feedforward: int = 512,\n",
        "                 dropout: float = 0.2):\n",
        "        super(Seq2SeqTransformer, self).__init__()\n",
        "        self.transformer = Transformer(d_model=emb_size,\n",
        "                                       nhead=nhead,\n",
        "                                       num_encoder_layers=num_encoder_layers,\n",
        "                                       num_decoder_layers=num_decoder_layers,\n",
        "                                       dim_feedforward=dim_feedforward,\n",
        "                                       dropout=dropout)\n",
        "        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n",
        "        self.src_tok_emb = TokenEmbedding(emb_size, src_embeddings)\n",
        "        self.tgt_tok_emb = TokenEmbedding(emb_size, tgt_embeddings)\n",
        "        self.positional_encoding = PositionalEncoding(\n",
        "            emb_size, dropout=dropout)\n",
        "\n",
        "    def forward(self,\n",
        "                src: Tensor,\n",
        "                trg: Tensor,\n",
        "                src_mask: Tensor,\n",
        "                tgt_mask: Tensor,\n",
        "                src_padding_mask: Tensor,\n",
        "                tgt_padding_mask: Tensor,\n",
        "                memory_key_padding_mask: Tensor):\n",
        "        src_emb = self.positional_encoding(self.src_tok_emb(src))\n",
        "        tgt_emb = self.positional_encoding(self.tgt_tok_emb(trg))\n",
        "        outs = self.transformer(src_emb, tgt_emb, src_mask, tgt_mask, None,\n",
        "                                src_padding_mask, tgt_padding_mask, memory_key_padding_mask)\n",
        "        return self.generator(outs)\n",
        "\n",
        "    def encode(self, src: Tensor, src_mask: Tensor):\n",
        "        return self.transformer.encoder(self.positional_encoding(\n",
        "                            self.src_tok_emb(src)), src_mask)\n",
        "\n",
        "    def decode(self, tgt: Tensor, memory: Tensor, tgt_mask: Tensor):\n",
        "        return self.transformer.decoder(self.positional_encoding(\n",
        "                          self.tgt_tok_emb(tgt)), memory,\n",
        "                          tgt_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "mT7MGqLIrN73",
      "metadata": {
        "id": "mT7MGqLIrN73"
      },
      "outputs": [],
      "source": [
        "def generate_square_subsequent_mask(sz):\n",
        "    mask = (torch.triu(torch.ones((sz, sz), device=DEVICE)) == 1).transpose(0, 1)\n",
        "    mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
        "    return mask\n",
        "\n",
        "\n",
        "def create_mask(src, tgt):\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    tgt_mask = generate_square_subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len),device=DEVICE).type(torch.bool)\n",
        "\n",
        "    src_padding_mask = (src == orth_alpha.get_stoi()[PAD]).transpose(0, 1)\n",
        "    tgt_padding_mask = (tgt == gloss_alpha.get_stoi()[PAD]).transpose(0, 1)\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "wu1XtSqprWSf",
      "metadata": {
        "id": "wu1XtSqprWSf"
      },
      "outputs": [],
      "source": [
        "\n",
        "torch.manual_seed(531)\n",
        "\n",
        "SRC_VOCAB_SIZE = len(orth_alpha)\n",
        "TGT_VOCAB_SIZE = len(gloss_alpha)\n",
        "EMB_SIZE = 300  # GloVe dim\n",
        "NHEAD = 6\n",
        "FFN_HID_DIM = 512\n",
        "NUM_ENCODER_LAYERS = 3\n",
        "NUM_DECODER_LAYERS = 3\n",
        "\n",
        "src_embeddings = nn.Embedding(SRC_VOCAB_SIZE, EMB_SIZE)\n",
        "tgt_embeddings = nn.Embedding(TGT_VOCAB_SIZE, EMB_SIZE)\n",
        "\n",
        "\n",
        "transformer = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, src_embeddings,\n",
        "                                 tgt_embeddings, FFN_HID_DIM)\n",
        "\n",
        "for p in transformer.parameters():\n",
        "    if p.dim() > 1:\n",
        "        nn.init.xavier_uniform_(p)\n",
        "\n",
        "transformer = transformer.to(DEVICE)\n",
        "\n",
        "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=0)\n",
        "\n",
        "optimizer = torch.optim.Adam(transformer.parameters(), lr=0.0001, betas=(0.9, 0.98), eps=1e-9)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "DBMe5q6Hrsdn",
      "metadata": {
        "id": "DBMe5q6Hrsdn"
      },
      "outputs": [],
      "source": [
        "\n",
        "def train_epoch(model, iterator, optimizer):\n",
        "    model.train()\n",
        "    losses = 0\n",
        "\n",
        "    for src, tgt in tqdm(iterator):\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(iterator)\n",
        "\n",
        "\n",
        "def evaluate(model, iterator):\n",
        "    model.eval()\n",
        "    losses = 0\n",
        "\n",
        "    for src, tgt in iterator:\n",
        "        src = src.to(DEVICE)\n",
        "        tgt = tgt.to(DEVICE)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]\n",
        "\n",
        "        src_mask, tgt_mask, src_padding_mask, tgt_padding_mask = create_mask(src, tgt_input)\n",
        "\n",
        "        logits = model(src, tgt_input, src_mask, tgt_mask,src_padding_mask, tgt_padding_mask, src_padding_mask)\n",
        "\n",
        "        tgt_out = tgt[1:, :]\n",
        "        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n",
        "        losses += loss.item()\n",
        "\n",
        "    return losses / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "id": "4lT_OG7arysQ",
      "metadata": {
        "id": "4lT_OG7arysQ"
      },
      "outputs": [],
      "source": [
        "from timeit import default_timer as timer\n",
        "\n",
        "def train_eval(NUM_EPOCHS, transformer, train_dataloader, optimizer, dev_dataloader, lang_code):\n",
        "    for epoch in range(1, NUM_EPOCHS + 1):\n",
        "        start_time = timer()\n",
        "        train_loss = train_epoch(transformer, train_dataloader, optimizer)\n",
        "        end_time = timer()\n",
        "        val_loss = evaluate(transformer, dev_dataloader)\n",
        "\n",
        "        state_dict_model = transformer.state_dict() \n",
        "\n",
        "        if epoch % 2 == 0:\n",
        "\n",
        "          torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': transformer.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'val_loss': val_loss,\n",
        "\n",
        "                }, \"/content/drive/MyDrive/2023glossingST/ckpt\" + lang_code + \"-\" + str(epoch))\n",
        "          print(f\"Checkpoint saved at epoch={epoch}\")\n",
        "\n",
        "        print((f\"Epoch: {epoch},  \"f\"Epoch time = {(end_time - start_time):.3f}s\"))\n",
        "        print(f'\\t Train Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "        print(f'\\t Val. Loss: {val_loss:.3f} |  Val. PPL: {math.exp(val_loss):7.3f}')\n",
        "\n",
        "    return transformer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2saPnTG1lTqo",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2saPnTG1lTqo",
        "outputId": "00547d73-0197-4860-c64f-f019cc00231a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1770/1770 [00:47<00:00, 37.09it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 1,  Epoch time = 47.730s\n",
            "\t Train Loss: 2.165 | Train PPL:   8.717\n",
            "\t Val. Loss: 1.456 |  Val. PPL:   4.288\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1770/1770 [00:48<00:00, 36.33it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved at epoch=2\n",
            "Epoch: 2,  Epoch time = 48.730s\n",
            "\t Train Loss: 1.498 | Train PPL:   4.474\n",
            "\t Val. Loss: 1.034 |  Val. PPL:   2.811\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1770/1770 [00:48<00:00, 36.46it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 3,  Epoch time = 48.559s\n",
            "\t Train Loss: 1.213 | Train PPL:   3.364\n",
            "\t Val. Loss: 0.780 |  Val. PPL:   2.181\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1770/1770 [00:47<00:00, 37.36it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved at epoch=4\n",
            "Epoch: 4,  Epoch time = 47.390s\n",
            "\t Train Loss: 1.015 | Train PPL:   2.760\n",
            "\t Val. Loss: 0.599 |  Val. PPL:   1.819\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1770/1770 [00:48<00:00, 36.24it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 5,  Epoch time = 48.852s\n",
            "\t Train Loss: 0.865 | Train PPL:   2.374\n",
            "\t Val. Loss: 0.483 |  Val. PPL:   1.620\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1770/1770 [00:48<00:00, 36.23it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved at epoch=6\n",
            "Epoch: 6,  Epoch time = 48.868s\n",
            "\t Train Loss: 0.746 | Train PPL:   2.109\n",
            "\t Val. Loss: 0.370 |  Val. PPL:   1.447\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1770/1770 [00:48<00:00, 36.48it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 7,  Epoch time = 48.531s\n",
            "\t Train Loss: 0.650 | Train PPL:   1.916\n",
            "\t Val. Loss: 0.294 |  Val. PPL:   1.342\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1770/1770 [00:47<00:00, 37.15it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved at epoch=8\n",
            "Epoch: 8,  Epoch time = 47.649s\n",
            "\t Train Loss: 0.571 | Train PPL:   1.769\n",
            "\t Val. Loss: 0.234 |  Val. PPL:   1.264\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1770/1770 [00:48<00:00, 36.70it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 9,  Epoch time = 48.243s\n",
            "\t Train Loss: 0.509 | Train PPL:   1.663\n",
            "\t Val. Loss: 0.190 |  Val. PPL:   1.209\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1770/1770 [00:48<00:00, 36.79it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved at epoch=10\n",
            "Epoch: 10,  Epoch time = 48.126s\n",
            "\t Train Loss: 0.454 | Train PPL:   1.574\n",
            "\t Val. Loss: 0.157 |  Val. PPL:   1.170\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1770/1770 [00:47<00:00, 37.59it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 11,  Epoch time = 47.101s\n",
            "\t Train Loss: 0.408 | Train PPL:   1.503\n",
            "\t Val. Loss: 0.134 |  Val. PPL:   1.143\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1770/1770 [00:48<00:00, 36.80it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved at epoch=12\n",
            "Epoch: 12,  Epoch time = 48.115s\n",
            "\t Train Loss: 0.369 | Train PPL:   1.446\n",
            "\t Val. Loss: 0.111 |  Val. PPL:   1.117\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1770/1770 [00:48<00:00, 36.65it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 13,  Epoch time = 48.302s\n",
            "\t Train Loss: 0.336 | Train PPL:   1.400\n",
            "\t Val. Loss: 0.096 |  Val. PPL:   1.101\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1770/1770 [00:47<00:00, 37.08it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved at epoch=14\n",
            "Epoch: 14,  Epoch time = 47.751s\n",
            "\t Train Loss: 0.306 | Train PPL:   1.358\n",
            "\t Val. Loss: 0.082 |  Val. PPL:   1.086\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1770/1770 [00:47<00:00, 37.55it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 15,  Epoch time = 47.148s\n",
            "\t Train Loss: 0.283 | Train PPL:   1.327\n",
            "\t Val. Loss: 0.076 |  Val. PPL:   1.079\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1770/1770 [00:47<00:00, 37.00it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved at epoch=16\n",
            "Epoch: 16,  Epoch time = 47.848s\n",
            "\t Train Loss: 0.260 | Train PPL:   1.297\n",
            "\t Val. Loss: 0.064 |  Val. PPL:   1.066\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1770/1770 [00:48<00:00, 36.52it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 17,  Epoch time = 48.470s\n",
            "\t Train Loss: 0.242 | Train PPL:   1.273\n",
            "\t Val. Loss: 0.057 |  Val. PPL:   1.058\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1770/1770 [00:47<00:00, 37.04it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved at epoch=18\n",
            "Epoch: 18,  Epoch time = 47.793s\n",
            "\t Train Loss: 0.225 | Train PPL:   1.253\n",
            "\t Val. Loss: 0.052 |  Val. PPL:   1.054\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1770/1770 [00:48<00:00, 36.45it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 19,  Epoch time = 48.578s\n",
            "\t Train Loss: 0.210 | Train PPL:   1.234\n",
            "\t Val. Loss: 0.048 |  Val. PPL:   1.049\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1770/1770 [00:48<00:00, 36.40it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved at epoch=20\n",
            "Epoch: 20,  Epoch time = 48.631s\n",
            "\t Train Loss: 0.197 | Train PPL:   1.217\n",
            "\t Val. Loss: 0.044 |  Val. PPL:   1.045\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1770/1770 [00:49<00:00, 36.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch: 21,  Epoch time = 49.018s\n",
            "\t Train Loss: 0.186 | Train PPL:   1.204\n",
            "\t Val. Loss: 0.039 |  Val. PPL:   1.040\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1770/1770 [00:48<00:00, 36.77it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Checkpoint saved at epoch=22\n",
            "Epoch: 22,  Epoch time = 48.150s\n",
            "\t Train Loss: 0.174 | Train PPL:   1.190\n",
            "\t Val. Loss: 0.036 |  Val. PPL:   1.037\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 31/1770 [00:01<01:01, 28.11it/s]"
          ]
        }
      ],
      "source": [
        "train_eval(40, transformer, train_dataloader, optimizer, dev_dataloader, \"lz\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wcyX8SQfveq-",
      "metadata": {
        "id": "wcyX8SQfveq-"
      },
      "outputs": [],
      "source": [
        "transformer_best = Seq2SeqTransformer(NUM_ENCODER_LAYERS, NUM_DECODER_LAYERS, EMB_SIZE,\n",
        "                                 NHEAD, SRC_VOCAB_SIZE, TGT_VOCAB_SIZE, src_embeddings,\n",
        "                                 tgt_embeddings, FFN_HID_DIM)\n",
        "\n",
        "transformer_best.load_state_dict(torch.load('/content/drive/MyDrive/2023glossingST/ckpt/tz80.pt')['state_dict'])\n",
        "transformer_best = transformer_best.to(DEVICE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "30-ZSS8dr8XQ",
      "metadata": {
        "id": "30-ZSS8dr8XQ"
      },
      "outputs": [],
      "source": [
        "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
        "    src = src.to(DEVICE)\n",
        "    src_mask = src_mask.to(DEVICE)\n",
        "\n",
        "    memory = model.encode(src, src_mask)\n",
        "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(DEVICE)\n",
        "    for i in range(max_len-1):\n",
        "        memory = memory.to(DEVICE)\n",
        "        tgt_mask = (generate_square_subsequent_mask(ys.size(0))\n",
        "                    .type(torch.bool)).to(DEVICE)\n",
        "        out = model.decode(ys, memory, tgt_mask)\n",
        "        out = out.transpose(0, 1)\n",
        "        prob = model.generator(out[:, -1])\n",
        "        _, next_word = torch.max(prob, dim=1)\n",
        "        next_word = next_word.item()\n",
        "\n",
        "        ys = torch.cat([ys,\n",
        "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=0)\n",
        "        if next_word == gloss_alpha.get_stoi()[END]:\n",
        "            break\n",
        "    return ys\n",
        "\n",
        "# actual function to translate input sentence into target language\n",
        "def translate(model: torch.nn.Module, src_tensor, extend_by=20):\n",
        "    model.eval()\n",
        "    src = src_tensor.view(-1, 1)\n",
        "    num_tokens = src.shape[0]\n",
        "    src_mask = (torch.zeros(num_tokens, num_tokens)).type(torch.bool)\n",
        "    tgt_tokens = greedy_decode(\n",
        "        model,  \n",
        "        src, \n",
        "        src_mask, \n",
        "        max_len=num_tokens + extend_by, \n",
        "        start_symbol=orth_alpha.get_stoi()[START]\n",
        "    ).flatten()\n",
        "\n",
        "    return re.sub(\"<start>|<end>\", \"\", ''.join([gloss_alpha.get_itos()[t] for t in tgt_tokens]))#\" \".join(vocab_transform[TGT_LANGUAGE].lookup_tokens(list(tgt_tokens.cpu().numpy()))).replace(\"<bos>\", \"\").replace(\"<eos>\", \"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BKbMPy_qwB1k",
      "metadata": {
        "id": "BKbMPy_qwB1k"
      },
      "outputs": [],
      "source": [
        "t=tensor_transform(lang.dev_splits)\n",
        "preds = [translate(transformer_best,o) for o in t]\n",
        "preds=[p.replace(\"--\",\"-\").replace(\"@\", \" ??? \").replace(\"#\", \" \") for p in preds]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YXAwkbsWupO9",
      "metadata": {
        "id": "YXAwkbsWupO9"
      },
      "outputs": [],
      "source": [
        "with open(\"/content/drive/MyDrive/2023glossingST/predictions/\" + \"tsez\", \"w\", encoding = \"utf-8\") as fout:\n",
        "      fout.write(\"\\n\".join(preds))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pubo4teIyYH9",
      "metadata": {
        "id": "pubo4teIyYH9"
      },
      "outputs": [],
      "source": [
        "langdict=lang.dev_ld"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "uCeuZtRLx0We",
      "metadata": {
        "id": "uCeuZtRLx0We"
      },
      "outputs": [],
      "source": [
        "pred_lines=[]\n",
        "for t, g,l in zip(langdict[\"transcriptions\"], preds, langdict[\"translation\"]):\n",
        "    pred_lines.append(\"\\\\t \" + t)\n",
        "    pred_lines.append(\"\\g \" + g)\n",
        "    pred_lines.append(\"\\l \" + l)\n",
        "    pred_lines.append(\" \")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XTtnqvlizAAj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTtnqvlizAAj",
        "outputId": "d0e246d4-1b76-48ca-9852-2d01d37ebcc8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['\\\\t « Зун », лагьана , « фена инсанрин арада гьатда , акван белке зи кьисметда ава .»',\n",
              " '\\\\g « And.but that , boy-AOR but , bat bathat be-ENT , « hat be-ENT hat be-ERG bring-DIR-GEN brie , bring-DAT bring-DIR-GEN bring-AOR , theathe.bris brin-IR-GENEN , say-AOR .',\n",
              " '\\\\l \"I will enter amongst the people.  Let me look - maybe it is my fate.\"',\n",
              " ' ',\n",
              " '\\\\t икьрар сад я .',\n",
              " '\\\\g boy-DIR-GEN brithat be-ENT boy-ERG , brithat be-ENT be-INESS , brithis be-INES be-INESS .',\n",
              " '\\\\l the decision is one',\n",
              " ' ',\n",
              " '\\\\t « Бес чун гьабурун патав хъфена кӀанда , Ша кӀватӀ хьана хъфин чун .»',\n",
              " '\\\\g « And.but that bot be-ENT that be-ENT bat , bat be-ENT hat be-ENT be-AOR , bring-D bring-DIR-GEN that bring-DAT bring-D , bring-D bring-DIR-GEN be-AT .',\n",
              " '\\\\l \"But we need to return to them. Come, let\\'s go back as a group.\"',\n",
              " ' ',\n",
              " '\\\\t Ашкъи вуш я , ашукь гада ?',\n",
              " '\\\\g boy-ERG that be-ENT this boy-ENT , brithat be-ENT be-INESS-SBS , brititis be-ENT be-INESS-S , brithis be-INES be-INEG .',\n",
              " '\\\\l What is love, hey ashuq',\n",
              " ' ',\n",
              " '\\\\t Вучиз , вутӀ хьана ?',\n",
              " '\\\\g but , bring-ERG be-ENT this be-ENT boy-DIR-GEN , be-ENT pie-INESS-S be-ELAT illie-G be-INEG , be-ENEG be-INESS-S .',\n",
              " '\\\\l “Why, where is it?”',\n",
              " ' ',\n",
              " '\\\\t Хъсан тостар лагьана , разивилер хьана , чун мерекадилай рази амукьна .',\n",
              " '\\\\g o-AOR this be-INESS bring-INESS , happpened-AOR , happpped-AOR be-AOR , hise-ENT brise-INESS peappppened-AOR be-INESS , hispened-AOR brispe .',\n",
              " '\\\\l It has been said many good toasts, everybody was happy about the event.',\n",
              " ' ',\n",
              " '\\\\t Ну Чукотка къийи чка тир , мекьи чка я .',\n",
              " '\\\\g this boy-DIR-GEN this brime-ENT this be-ENT , this be-ENT be-AOR this be-ENT , lothis be-INESS bris be-AOR .',\n",
              " '\\\\l And Chukotka was cold place',\n",
              " ' ',\n",
              " '\\\\t ( Натаван ).',\n",
              " '\\\\g boy-DIR-GEN be-INES bothis be-ENT boy-DIR-GEN be-AOR , brithat be-ENT be-EG be-EN , brithie-INESS .',\n",
              " '\\\\l Natavan',\n",
              " ' ',\n",
              " '\\\\t Абуру « эвера а гада » лугьуда , « абур ваз къачу » лугьуда , « абур адаз лайих туш ».',\n",
              " '\\\\g « this boy-ERG that boy-ENT that boy-ERG that , « that boy-ENT but t boy-ENT , brit be-ENT brithat , be-ENT brit boy-ENT , be-INEG this brit.brit.INT .',\n",
              " '\\\\l “Call the young man and take them for yourself. They are not fitting him.”',\n",
              " ' ',\n",
              " '\\\\t Ахпа и гадади лугьуда хьи , « бес гила вири хьанва », лугьуда , « бубадини жаза къачуна , муькуьбуруни жаза къачуна , бес ,» лагьана , « муькуь вилаятда зи дидени ама » лагьана , « пуд стхани ама , кьве стхани ама » лагьана .',\n",
              " '\\\\g « this boy-ERG say-ENT « that », say-ENT , « then t thioood br but , say-ENT , « one he t but hat but her-FOC but.AOC , 1sg.ERG there.abrin s , stion.AOR brin.ing-AOR , sthe.in.AOR sthe.brin.bring-AOR , ste.il.the.bre.brin.bring-AOR , s',\n",
              " '\\\\l Then, the guy said, \"Well, everything happened - Father was punished and others were punished. But my mother and two brothers are stll in the other province.\"',\n",
              " ' ',\n",
              " '\\\\t « Им ичӀерна гьа пачагьдин винелай лагь хьи » лугьуда , « вакай я кицӀ хьурай , я вак хьурай ».',\n",
              " '\\\\g « this boy-ERG that t boy-ENT that being , hat be-ENT his be-ENT boy-ENT , « hat t his be-ERG t time-ENT be-SBSS-INELAT this , tis be-ENT tis boy-ENT .',\n",
              " '\\\\l \"Pour it out on the king and say \\'May you be a dog, or may you be a pig!\\'\"',\n",
              " ' ',\n",
              " '\\\\t Мадни фена вич гьейетдал .',\n",
              " '\\\\g boy-DIR-GEN brithat be-ENT boy-DIR-GEN brit be-INESS , brithat be-INES be-FUT be-INEG .',\n",
              " '\\\\l He went to the courtyard again.',\n",
              " ' ',\n",
              " '\\\\t Ибурузни жаза це » лагьана .',\n",
              " '\\\\g boy-DIR-GEN blithat be-ENT be-ENT boy-ERG , brithat be-ENT be-INESS-SBS wit be-ENT .',\n",
              " '\\\\l \"They shall be punished, too\" she said.',\n",
              " ' ',\n",
              " '\\\\t Игирми уьч солдатдикай муьжуьд миллет авай чун , руссарни пара тир , муьжуьд миллет авай .',\n",
              " '\\\\g here-GEN this boy-DIR-GEN be-EN , look-INESS-SPS with be-ERG-GEN withat , be-ENT boy-ERG , brithis be-SBSS-ERG-GEN wis , boy-EN be-INT , this bor be-AOPSS wis .',\n",
              " '\\\\l There were eight nationalities out of twenty three soldiers, many russians.',\n",
              " ' ',\n",
              " '\\\\t « Гьуьлуьн кӀануз фин патакайни зун гьуьлуьн шивдихъ галаз ккӀана кӀанзава » лугьуда .',\n",
              " '\\\\g « this boy-DIR-GEN hat be-ENT his boy-DIR-GEN hat be-FOC but hat be-ENT , hise-ERG-GEN bring-DAT bot be-IR-DAT hat be-FOC this be-AOR , bris bring-DIR-GEN be-AT .',\n",
              " '\\\\l \"And to go to the bottom of the sea, I must fight with the Sea Steed.”',\n",
              " ' ',\n",
              " '\\\\t И чӀехи ваха гъвечидаз лагьана : Чан вах , лагьана , ви палкан гьич , лагьана .',\n",
              " '\\\\g he-GEN hims happened-AOR , happened-AOR , happened-AOR , hat Bat happened-ERG , hape-INESSS-SPSS wat , say-AOR , : happpened-PERF-NT .',\n",
              " '\\\\l The elder sister said to the younger : \"dear sister, your horse is nothing\"',\n",
              " ' ',\n",
              " '\\\\t стулар ахъайдай чилел , суфреярни , вири хуьруьн аялар фидай .',\n",
              " '\\\\g boy-FOC that , dago-AOC but , go-AOC brifter be-AOC , go-AOC buthat , bris girlll.be-INESS wath brifuter-ELAT water-ERG was ',\n",
              " '\\\\l they were serving tables on the ground [tablecloths?] and all the village children used to go there.',\n",
              " ' ',\n",
              " '\\\\t Элкъведа чун и севрхъ , чагъун тавур чаз .',\n",
              " '\\\\g this boy-DIR-GEN king bring-AOR , mother bring-AOC , that be-ENT be-ENT king-DIR-G king was .',\n",
              " \"\\\\l We looked for this bear but could't find him.\",\n",
              " ' ',\n",
              " '\\\\t Жи ашкъини сад я , мугьубатни сад я .',\n",
              " '\\\\g boy-FOC that be-AOC , that boy-ENT boy-ERG that be-AOR , that but be-ENT put boy-ENT .',\n",
              " '\\\\l We are in love with each other',\n",
              " ' ',\n",
              " '\\\\t Гьа~береда », лугьуда , « ам чукурда жемятди », лугьуда , « вун пачагь хкягъда » лугьуда .',\n",
              " '\\\\g « THIT.TEMPV this boy-ERG « that », say-ENT « that t t tey-ENT , « this boy-ENT t.AOR , « that t.INESS t.IMPV this boy-EN',\n",
              " '\\\\l \"At that time\" she said \"The people will chase it out and you will be chosen as king.\"',\n",
              " ' ',\n",
              " '\\\\t Зун фенавай Дербентдиз',\n",
              " '\\\\g 1pl.gen-DAT was be-ENT boy-DIR-GEN be-EN was be-ENT Gay-ENT be-ERG .',\n",
              " '\\\\l I had gone to Derbent',\n",
              " ' ',\n",
              " '\\\\t « АтӀутӀ ибурун кьилер » лагьана .',\n",
              " '\\\\g « AT.but boy-ENT that bein be-AOR boy-DIR-GEN be-G that be-EN .',\n",
              " '\\\\l “Cut off their heads!”',\n",
              " ' ',\n",
              " '\\\\t Ша экеча чал , Безханум',\n",
              " '\\\\g boy-DIR-GEN that be-AOR-D , boy-ENT be-ENT bat be-ENT , Bathat be-ENT be-AOR-G boy-ENT .',\n",
              " '\\\\l Come to the spring, Bezkhanum',\n",
              " ' ',\n",
              " '\\\\t « Накьвар », лагьана , « за гъана муьтлег ви вилер сагъарна кӀанда », лагьана .',\n",
              " '\\\\g « you », say-AOR , « thoo be-ERG hoood helse-GEN be-AOR , looood be-ENT-INESS how be-AOR , say-AOR , « deate.IMPV be-AOR doil',\n",
              " '\\\\l \"I must bring the soil and heal your eyes.” he said.',\n",
              " ' ',\n",
              " '\\\\t Къулухъ хтанва чун летний лагердиз , иддихатиз .',\n",
              " '\\\\g Afall bried.AOC but bring-INESS but , brith-DIR-DAT give-AOR bring-ENT , bring-ENT that be-FUT bris be-INES .',\n",
              " '\\\\l and returned back to the summer camp, to have a rest.',\n",
              " ' ',\n",
              " '\\\\t Исабала акурла лагьана хьи за , « Исабала стха , кӀурцӀуларни хана лугьузва , вахчама зи кицӀ .',\n",
              " '\\\\g he.at say-AOC thorse-DAT , one thorse-ERG-DAT one t , say-AOC , 1sg.ERG this , thouse-PL-ERG-DAT wout , thapppened-PERG-DAT on',\n",
              " '\\\\l When I saw Isabala I told him: “Brother Isabala, I heard the dog gave birth to the puppies. Return me the dog, please.”',\n",
              " ' ',\n",
              " '\\\\t Гьада мизан терезидив са пата эцигнава гьа инсан , мици патани – гьакъ , эдалет , дуьзвал , къенивал , михьивал .',\n",
              " '\\\\g this boy-DIR-DAT that , say-AOC , that , this boy-FOC , 1sg.ERG mother-ERG mon ther-DIR-DAT , one t sater-ERG-GEN come-FOC , s',\n",
              " '\\\\l She puts a person on the one plate of that scale and truth, justice, uprightness, purity on the another plate.',\n",
              " ' ',\n",
              " '\\\\t Идани авудна руш алцурарна вичин кӀвализ тухурла и руш катда идан гъиляй .',\n",
              " '\\\\g he.also-GEN this girl give-ENT houpppence-INESS this girl boy-DIR-DAT this girl boy-DIR-DAT this this boy-ERG thord this boy-',\n",
              " '\\\\l But when he disembarked her, deceived her, and was taking her to his house, the girl ran away from him.',\n",
              " ' ',\n",
              " '\\\\t Катана чна сада пенжере , акъатна инал , къазах тир',\n",
              " '\\\\g After be-AOC , this boook-AOC , brother be-AOC , that be-AOR , that be-AOR , this brome-ERG bring-AOR , bring-ENT-NT-NT .',\n",
              " '\\\\l We knocked a door and a kazakh man came out of the home',\n",
              " ' ',\n",
              " '\\\\t « Эвера », лагьана , « и гада къенин къалай жеда пачагь , зунни адан свас .»',\n",
              " '\\\\g « And.but , boy-DIR-GEN boy-DIR-GEN one be-FOC , hat boy-AOR , « king bring be-ENT bring-DIR-GEN , ode be-AOR bring-D , othe be-',\n",
              " '\\\\l \"Call the guy. From now on he will be the king and I will be his wife.\"',\n",
              " ' ',\n",
              " '\\\\t Эгьтибарсуз , вефасуз .',\n",
              " '\\\\g but , dagive-ERG be-ENT boy-ENT , brith-ERG be-ENT put be-INESS with-DAT .',\n",
              " '\\\\l unfaithful,  untrusted',\n",
              " ' ',\n",
              " '\\\\t Зунни эрменидихъ терсина',\n",
              " '\\\\g 1pl.gen babs go-AOR boy-DIR-GEN bring-DIR-GEN bat be-ENT girl be-ENT boy-EG .',\n",
              " '\\\\l but me and the armenian guy on the contrary',\n",
              " ' ',\n",
              " '\\\\t И~вахтунда пуд~лагьай гадади лагьана : « чан буба », лагьана , « гила за гъида », лагьана .',\n",
              " '\\\\g THIT.TEMP TEMP Thorse-ERG « how hat », say-AOR , « that t hat boy-AOR , « hat hat pener-ERG be-PENT », say-AOR .',\n",
              " '\\\\l ++At this time, the third son said: “Father, now I will bring it.\"',\n",
              " ' ',\n",
              " '\\\\t Бес , лагьана , къведай сеферда , лагьана , ша , лагьана , дидедиз са эрзе гун ма чна .',\n",
              " '\\\\g but , say-AOC , , but , but , say-AOC , but , but , but , but but , say-AOC , but but , but , but bris brit , bristeay-AOC , bristher-ERG bristeay-AO',\n",
              " '\\\\l But asked to come next time and give a petition to mother',\n",
              " ' ',\n",
              " '\\\\t живедин цӀуьлуь пилте ма , пилте',\n",
              " '\\\\g boy-DIR-GEN thappened-AOR boy-DIR-GEN be-EN , boy-ENT be-FOC be-ENT be-ENT pithis be-G , boy-ENT be-INESS-SS-SPS .',\n",
              " '\\\\l Heavy flakes of snow.',\n",
              " ' ',\n",
              " '\\\\t Ви рикӀал аламани ?)',\n",
              " '\\\\g boy-DIR-GEN boy-EN be-INESS pithapppen be-AOR , boy-ENT be-INESS be-ELAT .',\n",
              " '\\\\l Do you remember?',\n",
              " ' ',\n",
              " '\\\\t Ну , тарихдин гьи делилар лагьайла , тарихдин точно~дан , абур ма — гьым , гьым , вахт завай лугьуз жедач , чӀавар къалур жедач завай квез .',\n",
              " '\\\\g 1pl.gen , say-AOC , That , 1sg.abs one capper.AOC , omount chappened-PL-ERG-DAT , one come-IMPV , say-AOC , 1pl.abs one ce ce ing clive-PL-ST , ster.',\n",
              " '\\\\l Well…in regard to the evidences of history, exactly, they yea…hmm…hmm, I can’t tell the time of the history, can’t show you the times.',\n",
              " ' ',\n",
              " '\\\\t Зун а Советрин девирдиз партиядиз кьабулна , заз отпуска гана десять сутка , ихьтинбур хьана .',\n",
              " '\\\\g 1sg.ERG-FOC that t that boodat ke.say-AOC , 1sg.ERG that t thes broodat , ther-ERG-GEN take-EN thapppppened-AOR , thak thise.sak brat , the.say-AOC .',\n",
              " '\\\\l I was taken to a party, I got 10 days of vacation, and all these happened during the Soviet government',\n",
              " ' ',\n",
              " '\\\\t Бес , лагьана , са агъуз плотинадал фин чун .',\n",
              " '\\\\g but , say-AOR , but bring-AOR , but bring-AOR , but , bring-ENT be-ENT , say-AOR bring-DIR-GEN king-DAT king-D .',\n",
              " '\\\\l he decided to go down to the dam.',\n",
              " ' ',\n",
              " '\\\\t Гьанал чна , зун , зун башда олмагла ...',\n",
              " '\\\\g but , say-AOR , that be-ENT , brimat be-ERG , this be-ENT , be-ENT ligive-EG be-ENT-ENT .',\n",
              " '\\\\l nan',\n",
              " ' ',\n",
              " '\\\\t Ахпа винел чи ам хтана , капитан .',\n",
              " '\\\\g boy-DIR-GEN-ERG that be-AOR , this bout be-ENT , happened-AOR be-ENT-ENT be-AOR , this bris be-ENT .',\n",
              " '\\\\l Then our captain returned.',\n",
              " ' ',\n",
              " '\\\\t « Самур » газетдин къула къени цай авазва .',\n",
              " '\\\\g « AT but bothat be-INESS boy-DIR-DAT plat be-IR-GEN hat be-AOR puat be-INESS be-INESS purn-PSS pis be-INESS be-AOR .',\n",
              " '\\\\l The fire in \"Samur\"newspaper is still existed',\n",
              " ' ',\n",
              " '\\\\t куьтях хьана ... гьанал чун минометчикар тир , минаяр вегьидай пуд нефер',\n",
              " '\\\\g this boy-DIR-GEN happened-AOR thappened-AOR thappened-AOR , thapped-AOR thappened-AOR t , thappened-AOR thapppped-AOR brace-ENT-T .',\n",
              " '\\\\l . then it finished... We were mortar men, there were three of us.',\n",
              " ' ',\n",
              " '\\\\t Ибуруни им тада анал и вичин тахтунал ацукьарда .',\n",
              " '\\\\g here-GEN this boy-DIR-GEN thappen boat be-ENT happpened-AOR that be-ENT-ENT his broat , ther be-ENT this be-AOR .',\n",
              " '\\\\l And they sat him and her on a throne.',\n",
              " ' ',\n",
              " '\\\\t Рекьикь ама !',\n",
              " '\\\\g boy-DIR-GEN be-AT this boy-DIR-GEN be-AOR this be-GEN , boy-EN be-INESS be-ELAT .',\n",
              " '\\\\l nan',\n",
              " ' ',\n",
              " '\\\\t Эверда и луьтквечидиз лугьуда хьи , « чан стха , чун » лугьуда « гьуьлуьн тӀва къерехдиз акъудна кӀанда вуна ».',\n",
              " '\\\\g « now-INESS how-INES t tood happpened-PERF », say-ENT « that 1sg.abs hat bout king.IMPF-PTP that , boy-ERG « that t t bout.IR-DAT ke-AOR put.IR-GENEN pu',\n",
              " '\\\\l They called this boatman and asked \"Brother, you must take us to the other side of the sea.\"',\n",
              " ' ',\n",
              " '\\\\t Чан Безханум , чан Безханум',\n",
              " '\\\\g Bazku-INES be-INES , Bezkhapppenen baing-INESS , Bezkiman be-AOR , Bead be-ENT be-AOR , Beaillllllllllllllkik be-INEG be-EG .',\n",
              " '\\\\l Dear Bezkhanum, dear Bezkhanum',\n",
              " ' ',\n",
              " '\\\\t КӀватӀна инсанар эверда вири .',\n",
              " '\\\\g boy-DIR-GEN that boy-DIR-GEN boy-DIR-GEN bat be-INESS-SPS withat be-ENT , boy-ENT be-INELAT .',\n",
              " '\\\\l All the people were called and gathered.',\n",
              " ' ',\n",
              " '\\\\t Цвез гуьре къунва ?',\n",
              " '\\\\g boy-DIR-GEN boy-ERG be-INES pith-DAT boy-ERG be-FOC .',\n",
              " '\\\\l Why is she guided by it?',\n",
              " ' ',\n",
              " '\\\\t И~вахтунда пуд~лагьай гадади лагьана : « чан буба », лагьана , « гила за гъида », лагьана .',\n",
              " '\\\\g THIT.THIMPV TEMPV TONT « 1sg.ERG horse-ERG » say-AOR , « 1sg.ERG 2sg-DAT », say-AOR , « what.INESS wi',\n",
              " '\\\\l ++At this time, the third son said: “Father, now I will bring it.\"',\n",
              " ' ',\n",
              " '\\\\t зунни фу тӀуьна , чна пуда , зунни чал фена .',\n",
              " '\\\\g 1sg.abs-FOC that , 1pl.abs go-AOR bsay-AOR , that be-ERG go-AOR deag be-ENT , go-AOR deago-AOR .',\n",
              " '\\\\l We had food and I went to drink some water.',\n",
              " ' ',\n",
              " '\\\\t « Пара вун фикирдиз фенва хьи ».',\n",
              " '\\\\g « AT.T.THIMPV that boy-DIR-GEN boy-DIR-GEN be-FOC give-INESS be-AOR .',\n",
              " '\\\\l \"You are too thoughtful.”',\n",
              " ' ',\n",
              " '\\\\t Зу мама муеллиме тир , зу бубани бухгалтер тир .',\n",
              " '\\\\g 1sg.ERG thes deat brieat , deas be-ERG deagive-ENT-AOP thes be-ENT .',\n",
              " '\\\\l My mother was a teacher, my father was an accountant.',\n",
              " ' ',\n",
              " '\\\\t ахпа хтана свободный , затӀдин , семьядиз',\n",
              " '\\\\g After frone be-AOC , that be-AOC , dagrod be-INESS , that be-AOC dat be-INESS , dasthere-INESS .',\n",
              " \"\\\\l and then I returned, i'm free....\",\n",
              " ' ',\n",
              " '\\\\t Сад гагьатзава .',\n",
              " '\\\\g SUND papeappeappeappeappeas be-AOR Top Gapppeape-INESS G .',\n",
              " '\\\\l Somebody runs',\n",
              " ' ',\n",
              " '\\\\t икьрардилай зун чира туш , лугьузва .',\n",
              " '\\\\g boy-DIR-GEN-ERG this boy-DIR-GEN , happened-AOR be-ENT-ENT , this be-ENT be-EG isigive-AOR .',\n",
              " \"\\\\l I haven't changed my mind\",\n",
              " ' ',\n",
              " '\\\\t Аналай сонра заз досрочный звание сержант гана .',\n",
              " '\\\\g Afrer falll be-AOC 1sg.abs AOC Aforother bring-INESS foron.be-AOC .',\n",
              " '\\\\l I was promoted to Sergeant earlier than anticipated',\n",
              " ' ',\n",
              " '\\\\t ГьакӀ хьайила ашукъди вуш лугьузва ?',\n",
              " '\\\\g boy-ERG that be-ENT boy-ERG this boy-ERG be-ENT boy-ENT-ENT be-FOC boy-ENT .',\n",
              " '\\\\l And what ashuq says',\n",
              " ' ',\n",
              " '\\\\t Хъфида зун чубандин гьейетдал .',\n",
              " '\\\\g o-AOR TEMP-DIR-GEN perbe-AOR pear be-INESS Gepperbe-AOR pupppen-DIR-GEN .',\n",
              " '\\\\l I went to the shepherds courtyard again.',\n",
              " ' ',\n",
              " '\\\\t Ваъ , гьуьлуьн винел иви акъат тавуртӀа , зун амач », лугьуда .',\n",
              " '\\\\g but , say-AOR , « 1sg.abs bs hood give-ERG happened-PERF , looo.d hat be-ENT hat , dooo.do.IMPV be-INE',\n",
              " '\\\\l \"But if there is no blood on the surface, it means I dont exist anymore.” said the horse.',\n",
              " ' ',\n",
              " '\\\\t Атайла , « я гада », лугьуда , « заз ван атана , лагьана , ви кӀвале гевгьерар ава » лагьана .',\n",
              " '\\\\g « come-PST », say-AOR , « 1sg.ERG 2sg-DAT 2sg-DAT », say-AOR , « 2sg-DAT hel.gen come cop », say-AOR , «',\n",
              " '\\\\l When he came, the king said, “Oh boy, word has come to me that at your house are pearls\"',\n",
              " ' ',\n",
              " '\\\\t Анай сонра фен чун Чукоткадиз .',\n",
              " '\\\\g After boy-AOC 1pl.abs gen brothes go-AOR bring-DIR-DAT go-AOR bring.be-AOR .',\n",
              " '\\\\l Then we went to Chukotka',\n",
              " ' ',\n",
              " '\\\\t Амай жамаатдилай Аллагь рази хьуй .',\n",
              " '\\\\g boy-DIR-GEN his be-GEN boy-DIR-GEN be-AOP pith-DIR-GEN be-AT pis be-ENT-ENT .',\n",
              " '\\\\l And may God bless the others.',\n",
              " ' ',\n",
              " '\\\\t Bун закай , зун вакай инжиклу тир вахтарни хьана .',\n",
              " '\\\\g here-DAT-FOC , love-IMPF-PTP-TEMP thappened-AOR be-ERG-GEN boy-AOR be-ERG-GEN be-AOR this be-PERF-PTP-SBSBSBST w',\n",
              " '\\\\l sometimes we were offended by each other. nhbojkmhj',\n",
              " ' ',\n",
              " '\\\\t Ана .',\n",
              " '\\\\g boy-DIR-GEN be-AT pith-ENT boy-ERG be-INES G be-ENT .',\n",
              " '\\\\l nan',\n",
              " ' ',\n",
              " '\\\\t А берейра вообще дегьшет тир .',\n",
              " '\\\\g boy-DIR-GEN that be-ENT this boy-DIR-GEN be-AOP-SBST withat be-ENT be-ENT .',\n",
              " '\\\\l But in the past it was awesome.',\n",
              " ' ',\n",
              " '\\\\t И акъудир кьван , чна чай акъудир кьван чӀавал .',\n",
              " '\\\\g he-GEN this boy-DIR-GEN , happenened-AOR this bout , look-AOR be-ENT-ENT-EG hise-SBST wappeneaped-AOR .',\n",
              " '\\\\l Untill we put him out of water.',\n",
              " ' ',\n",
              " '\\\\t Ибуруни тухуда и шикил эцигда булахдал .',\n",
              " '\\\\g this boy-ENT this boy-ENT this boy-ERG this bring.ENT this boy-ENT tat.ENT boy-ENT .',\n",
              " '\\\\l And they took the picture and placed at the spring.',\n",
              " ' ',\n",
              " '\\\\t Гьа икӀ , са гада акуна заз чибриз ухшар я чинай са тӀимил~кьван .',\n",
              " '\\\\g 1sg.ERG-DAT this boy-DAT , 1pl.gen-DAT was happpen-DAT AT inen-D.T THEMP-TEMP THEMP-SBST-T TEMP T happene was be-AOC .',\n",
              " '\\\\l So, I saw a young man whose face looked like ours a little.',\n",
              " ' ',\n",
              " '\\\\t Гададини фикирда , « яраб » лугьуда « зун фейитӀа эвел руш инал туна хьуй бубади атана руш чуьнуьх хъийида .',\n",
              " '\\\\g « go-AOR this go-AOR boy-DIR-GEN « girl give-ENT », say-ENT « the.at tood.IMPV girl girl be.IMPV bouting-DIR-GEN bord.IR-GE',\n",
              " '\\\\l The guy thought \"I wonder - if I leave the girl here and go first, my father will come and steal her.\"',\n",
              " ' ',\n",
              " '\\\\t Эхир пара фикирайдалай сонра руш ацукьарна луьткведа , ракъуда и руш луьткведаваз тӀва патахъ .',\n",
              " '\\\\g this girl give-AOC this boy-DIR-GEN ood girl booad ging.AOC thout.ENT-ENT , this girl bout.AOC , thout.ordow tak.ENT o',\n",
              " '\\\\l In the end, after many thoughts, he boarded the girl and sent her by boat to the other side.',\n",
              " ' ',\n",
              " '\\\\t Эверда и луьтквечидиз лугьуда хьи , « чан стха , чун » лугьуда « гьуьлуьн тӀва къерехдиз акъудна кӀанда вуна ».',\n",
              " '\\\\g « now-INESS how-INES t.and how-INES » say-ENT , « 1sg.abs 2s 2sg-DAT that bse.ing-DIR-GEN », say-ENT « 1sg.abs wabs chomut',\n",
              " '\\\\l They called this boatman and asked \"Brother, you must take us to the other side of the sea.\"',\n",
              " ' ',\n",
              " '\\\\t Чун къвед гагьатна .',\n",
              " '\\\\g boy-DIR-GEN happened-AOR boy-DIR-GEN be-EN pop be-ENT G piaperace-AOR .',\n",
              " '\\\\l Both of us ran away.',\n",
              " ' ',\n",
              " '\\\\t 04 : Ӏ9',\n",
              " '\\\\g lat ake-INESS ppeappear pappeaas 0 : 04 : 04',\n",
              " '\\\\l nan',\n",
              " ' ',\n",
              " '\\\\t Гьа~береда », лугьуда , « ам чукурда жемятди », лугьуда , « вун пачагь хкягъда » лугьуда .',\n",
              " '\\\\g « THIT.TEMPV this boy-ERG « that », say-ENT « that t , « king-ENT t.INES that t boy-ENT , « t.INES that t boy-ES t.',\n",
              " '\\\\l \"At that time\" she said \"The people will chase it out and you will be chosen as king.\"',\n",
              " ' ',\n",
              " '\\\\t Батальондин геодезист тир зун',\n",
              " '\\\\g but TEMP-DIR-GEN penid bring-DIR-DAT GEN bring-DIR-GEN be-AOR , dead be-ENT Gerigirllage-INESS GENES Gerove-INES .',\n",
              " '\\\\l I was the land surveyor of our battalion.',\n",
              " ' ',\n",
              " '\\\\t Майдиз лишанлу хьана , Сентябрдиз чун эвли хьана .',\n",
              " '\\\\g 1sg.ERG thapppened-AOR bring-POBL-DAT , bathappened-AOR be-ENT-FUT brith-FUT , brig.athappped-PERF be-INESS .',\n",
              " '\\\\l We engaged in May and married in September',\n",
              " ' ',\n",
              " '\\\\t « АкӀ жеш хьи », лугьуда .',\n",
              " '\\\\g « that be-ENT boy-ENT this boy-ERG be-ENT , boy-ENT be-INESS-SS-SBS .',\n",
              " '\\\\l \"It cannot work this way.\"',\n",
              " ' ',\n",
              " '\\\\t КӀанивилиз идан цӀийи кьиляй манияр туькӀуьрна Безханумаз :',\n",
              " '\\\\g Bazkhapppened-AOR Thapppened-INESS was Bezkhapppen-DIR-DAT T Thapppened-PERF happppened-PERF be-AOR .',\n",
              " '\\\\l And he wrote a new love song to Bezkhanum',\n",
              " ' ',\n",
              " '\\\\t Стхарихъ галаз мегьрибан хьана , хъсан хьана .',\n",
              " '\\\\g Sefappened-AOC Thappened-AOR , Thapped-AOR be-FUT T Tappened-AOR be-AOR , as brithappped-PERF .',\n",
              " '\\\\l We were very friendly, and kind.',\n",
              " ' ',\n",
              " '\\\\t Зун , лагьана , куьмек ая , зун хутах , лагьана , инара тамир , лагьана .',\n",
              " '\\\\g 1sg.abs , say-AOC , 1sg.abs , hat happened-AOR , say-AOR , 1sg.abs hat , say-AOR , deat de.say-AOR , deat deck happpe.be-ERG',\n",
              " '\\\\l He asked to help him and to take him back.',\n",
              " ' ',\n",
              " '\\\\t Вичин хва рекьиз кӀан жени ?',\n",
              " '\\\\g boy-DIR-GEN-ERG hat be-ERG-GEN be-AOR willl be-ERG-GEN be-EN be-Q be-ENT igirllll.be-INELAT be-FUT .',\n",
              " '\\\\l Would she ever want her son to be killed?',\n",
              " ' ',\n",
              " '\\\\t Бес , лугьузва , Безханум , лугьузва , дуьняда вири са пад хьайтӀани , ашукьдиз вуна гаф гана кӀанда .',\n",
              " '\\\\g but , say-AOC , , 1sg.abs , sabs but , say-IMPF , Bear-ERG but , say-AOC , but , say-IMPF , but be-INESS word be-PST , say-AOC',\n",
              " '\\\\l And you must give a word to ashuq, even if the whole world will be agains you',\n",
              " ' ',\n",
              " '\\\\t « Чан къари баде » лугьуда , « ваз руш кӀандани », лугьуда , « ваз са бала кӀандани », лугьуда ?',\n",
              " '\\\\g « now-INES boy-ERG say-ENT « ond but hat boy-ENT , « 1sg.abs hat boy-ERG but , hat but girde.IMPV but , hat but.IMPV but.IM',\n",
              " '\\\\l \"Dear old grandmother, don’t you need a daughter - don\\'t you want a child?\" the girl said.',\n",
              " ' ',\n",
              " '\\\\t Ахъаюнмаз лув~гана атана и къуш ацукьда рушан къуьнел .',\n",
              " '\\\\g AT.TIMMMAOME.T This boy-DIR-GEN happened-AOR this boy-ENT-ENT this boy-ERG this boy-ENT .',\n",
              " '\\\\l As soon it was released, this bird flew over and sat on the girl’s shoulder.',\n",
              " ' ',\n",
              " '\\\\t « Исятда гъишни за ваз », лугьуда , « захъ ктадмир анжах ».',\n",
              " '\\\\g « IMPV that boy-ERG « that boy-ENT bring-DIR-GEN bring-DAT , king-Q king-ENT be-ENT , dowat be-ENT be-INES brith-DAT .',\n",
              " '\\\\l \"I will bring them to you now. Only please don\\'t touch me.”',\n",
              " ' ',\n",
              " '\\\\t Чун хтана и са ана хуьруз',\n",
              " '\\\\g boy-FOC 1pl.abs bride be-AOP-TEMP-TEMP-TEMP-DIR-GEN brithat be-ENT be-ENT , boy-ERG brithe be-AOR .',\n",
              " '\\\\l We reached a village there',\n",
              " ' ',\n",
              " '\\\\t БалкӀанди лугьуда : « вуч хьанва », лугьуда , « мадни кьил кудна хьи », лугьуда , « вуч хьанва , вуч фикирарзава ?»',\n",
              " '\\\\g « horse-ERG hat », say-ENT « hor hat he.what happened-PERF », say-ENT , « hat hat hat hat hed-PERF hat happened-PERF », say-E',\n",
              " '\\\\l The horse asked, “What happened? Your head is down again. What happened? What are you thinking about?”',\n",
              " ' ']"
            ]
          },
          "execution_count": 183,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pred_lines"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nurKbItPBYTu",
      "metadata": {
        "id": "nurKbItPBYTu"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}

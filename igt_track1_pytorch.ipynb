{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "928224ef-bffd-46b8-b59d-560a26fec08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import numpy as np\n",
    "import math\n",
    "from collections import defaultdict, Counter, OrderedDict\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from torch.utils.data import DataLoader\n",
    "from torchtext.vocab import build_vocab_from_iterator, vocab\n",
    "\n",
    "from itertools import chain\n",
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "## other imports \n",
    "import io\n",
    "import spacy\n",
    "import numpy as np\n",
    "import spacy.cli\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "9a957e06-a7c1-420f-a588-2dfa79c979d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, Tensor\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import TransformerEncoder, TransformerEncoderLayer\n",
    "from torch.utils.data import dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "13aa174a-412c-4945-a6b7-85da2d6e113b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize = lambda x: re.sub(\"'\",\"\",normalizer.normalize_str(x))\n",
    "\n",
    "START = \"<start>\"\n",
    "END = \"<end>\"\n",
    "UNK = \"<unk>\"\n",
    "PAD = \"<pad>\"\n",
    "BD = \"<bd>\"\n",
    "SPECIALS = [START,END,UNK,PAD, BD]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "0d1f0d13-c1d2-43c8-8024-9b844a87ce4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "langs_codes = {\"Gitksan\": \"git\", \"Arapaho\":\"arp\", \"Lezgi\":\"lez\", \"Nyangbo\":\"nyb\", \"Tsez\":\"ddo\",\"Uspanteko\":\"usp\"}\n",
    "\n",
    "def tag_bd(seq):\n",
    "    for i, c in enumerate(seq):\n",
    "        if c == \"-\":\n",
    "            seq[i] = \"<bd>\"\n",
    "    return seq\n",
    "\n",
    "def get_lang_df(lang):\n",
    "    ld = {\n",
    "        \"train\": defaultdict(list),\n",
    "        \"dev\": defaultdict(list)\n",
    "    }\n",
    "        \n",
    "    path = f\"data/{lang}/\"\n",
    "    train_fn = f\"{langs_codes[lang]}-train-track2-uncovered\"\n",
    "    dev_fn = f\"{langs_codes[lang]}-dev-track2-uncovered\"\n",
    "\n",
    "    for fp in [train_fn, dev_fn]:\n",
    "        data_type = fp.split(\"-\")[1]\n",
    "        with open(path + fp, \"r\") as f:\n",
    "            for line in f.readlines():\n",
    "                \n",
    "\n",
    "                if line.startswith(\"\\\\t\"):\n",
    "                    ld[data_type][\"transcription\"].append(line.lstrip(\"\\\\t \").rstrip(\"\\n\"))\n",
    "                if line.startswith(\"\\g\"):\n",
    "                    ld[data_type][\"glosses\"].append(line.lstrip(\"\\\\g \").rstrip(\"\\n\"))\n",
    "                if line.startswith(\"\\m\"):\n",
    "                    ld[data_type][\"morphemes\"].append(line.lstrip(\"\\\\m \").rstrip(\"\\n\"))\n",
    "\n",
    "                if line.startswith(\"\\l\"):\n",
    "                    ld[data_type][\"translation\"].append(normalize(line.lstrip(\"\\\\l \").rstrip(\"\\n\")))\n",
    "    \n",
    "    train_df = pd.DataFrame.from_dict(ld[\"train\"])\n",
    "    dev_df = pd.DataFrame.from_dict(ld[\"dev\"])\n",
    "    \n",
    "    return ld[\"train\"],ld[\"dev\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "a4318579-c164-4605-864c-4601acecd79b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_segmented(line):\n",
    "    BD = \"#\"\n",
    "    allsplits=[]\n",
    "\n",
    "    splits=[m.split() for m in line]\n",
    "    for i in splits:\n",
    "        for m in i:\n",
    "            allsplits.append(m)\n",
    "    return allsplits\n",
    "\n",
    "def get_segmented_data(langdict,splits=[\"morphemes\",\"glosses\"]):\n",
    "    data = {}\n",
    "    for split in splits:\n",
    "        data[split] = get_segmented(langdict[split])\n",
    "    return data\n",
    "\n",
    "def get_characterized_data(segments,splits=[\"morphemes\",\"glosses\"]):\n",
    "    data = {}\n",
    "    for split in splits:\n",
    "        data[split] = [[START] + tag_bd(list(m)) + [END] for m in segments[split]]\n",
    "    return data\n",
    "\n",
    "def get_train_val_split(data, props):\n",
    "    test_data = []\n",
    "    train_data = []\n",
    "    for i, dat in enumerate(data):\n",
    "        if i%props == 0:\n",
    "            test_data.append(dat)\n",
    "        else:\n",
    "            train_data.append(dat)\n",
    "    return test_data, train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "2ea9fa1e-e9c3-42b8-90d6-890d494ae579",
   "metadata": {},
   "outputs": [],
   "source": [
    "train,dev = get_lang_df(\"Arapaho\")\n",
    "train_morphs, train_glosses = train[\"morphemes\"], train[\"glosses\"]\n",
    "dev_morphs, dev_glosses = dev[\"morphemes\"], dev[\"glosses\"]\n",
    "\n",
    "voc = build_vocab_from_iterator(chain(train_morphs,train_glosses), specials = SPECIALS, special_first=True)\n",
    "voc.set_default_index(voc[UNK])\n",
    "transform_sequence = lambda x: [voc[c] for c in x]    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "33c950e6-124a-48e5-b4a3-7b8aaf187f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    \n",
    "    input, output = [], []\n",
    "    for morphemes, glosses in batch:\n",
    "        input_tensor = torch.tensor(transform_sequence(morphemes), dtype=torch.long)\n",
    "        output_tensor = torch.tensor(transform_sequence(glosses), dtype=torch.long)\n",
    "        input.append(input_tensor)\n",
    "        output.append(output_tensor)\n",
    "        \n",
    "\n",
    "    return pad_sequence(input, batch_first=False, padding_value=voc[PAD]), pad_sequence(output, batch_first=False, padding_value=voc[PAD])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "a47a3a9b-f7d9-4069-9a5d-36e13d39462b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_processed = get_characterized_data(get_segmented_data(train))\n",
    "dev_processed = get_characterized_data(get_segmented_data(dev))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "041af526-232d-491f-a651-b9faacf5c306",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_data = [(morph, gloss)for morph, gloss in zip(train_processed[\"morphemes\"], train_processed[\"glosses\"])]\n",
    "dev_data = [(morph, gloss)for morph, gloss in zip(dev_processed[\"morphemes\"], dev_processed[\"glosses\"])]\n",
    "\n",
    "test_data, train_data = get_train_val_split(train_test_data, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b782eb83-4940-4321-b255-2a19376ba86d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train_dataloader = DataLoader(\n",
    "    train_data, \n",
    "    batch_size=1,\n",
    "    collate_fn=collate_batch,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "dev_dataloader = DataLoader(\n",
    "    dev_data, \n",
    "    batch_size=1,\n",
    "    collate_fn=collate_batch,\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_data, \n",
    "    batch_size=1,\n",
    "    collate_fn=collate_batch,\n",
    "    shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a284b560-ffb5-4bb7-abd3-10c08f39efd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor size of source language: torch.Size([7, 1])\n",
      "tensor size of target language: torch.Size([6, 1])\n",
      "the tensor of first example in target language: tensor([ 0, 17, 13,  8, 17,  1])\n",
      "the tensor of first example in src language: tensor([ 0, 13,  7, 16, 15, 15,  1])\n",
      "['<start>', 'h', 'i', 'n', 'e', 'e', '<end>']\n",
      "['<start>', 't', 'h', 'a', 't', '<end>']\n"
     ]
    }
   ],
   "source": [
    "for batch in train_dataloader:\n",
    "    src, trg = batch\n",
    "    print('tensor size of source language:', src.shape)\n",
    "    print('tensor size of target language:', trg.shape)\n",
    "    print('the tensor of first example in target language:', trg[:, 0])\n",
    "    print('the tensor of first example in src language:', src[:, 0])\n",
    "    print([voc.get_itos()[i] for i in src[:, 0]])\n",
    "    print([voc.get_itos()[i] for i in trg[:, 0]])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "999f3e46-93fe-4a5c-b83b-a7d6da1f4aaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open(\"torchdata/train_data\", \"wb\") as f:\n",
    "     pickle.dump(train_data, f)\n",
    "\n",
    "with open(\"torchdata/val_data\", \"wb\") as f:\n",
    "     pickle.dump(dev_data, f)\n",
    "\n",
    "with open(\"torchdata/test_data\", \"wb\") as f:\n",
    "     pickle.dump(test_data, f)\n",
    "        \n",
    "with open(\"torchdata/vocab\", \"wb\") as f:\n",
    "     pickle.dump(voc, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "dffc2fef-d70f-4648-90d9-ec3665cc3900",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed(0)\n",
    "torch.manual_seed(0)\n",
    "np.random.seed(0)\n",
    "\n",
    "import re\n",
    "\n",
    "# Hyperparameters\n",
    "EMBEDDING_DIM=50\n",
    "RNN_HIDDEN_DIM=50\n",
    "RNN_LAYERS=1\n",
    "BATCH_SIZE=10\n",
    "CHAR_DROPOUT=0.2\n",
    "EPOCHS=10\n",
    "\n",
    "# Maximum length of generated output word forms.\n",
    "MAXWFLEN=40\n",
    "\n",
    "def accuracy(sys,gold):\n",
    "    assert(len(sys) == len(gold))\n",
    "    return sum([1 if x==y else 0 for x,y in zip(sys,gold)])*100.0/len(gold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "eb1e0050-6d81-42ab-81f4-5856c1909620",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "        def __init__(self,alphabet):\n",
    "                super(Encoder,self).__init__()\n",
    "                self.embedding = nn.Embedding(len(alphabet), EMBEDDING_DIM)\n",
    "                self.rnn = nn.LSTM(EMBEDDING_DIM, RNN_HIDDEN_DIM, RNN_LAYERS, bidirectional=True)\n",
    "\n",
    "        def forward(self,ex):\n",
    "            input, _ = ex\n",
    "            encoder_embedded = self.embedding(input)\n",
    "            encoder_output, (hn,cn) = self.rnn(encoder_embedded)\n",
    "            return encoder_output\n",
    "\n",
    "# An assertion to test that your implementation returns an object of the correct size. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "4ecf5756-182f-45b5-83e9-af367c2ad76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, alphabet):\n",
    "        super(Decoder,self).__init__()\n",
    "        self.alphabet = alphabet\n",
    "        self.embedding = nn.Embedding(len(alphabet), EMBEDDING_DIM)\n",
    "        self.attention = Attention()\n",
    "        self.rnn = nn.LSTM(EMBEDDING_DIM+2*RNN_HIDDEN_DIM, RNN_HIDDEN_DIM, RNN_LAYERS, bidirectional=False)\n",
    "        self.hidden2char = nn.Linear(RNN_HIDDEN_DIM, len(alphabet))\n",
    "    \n",
    "    def forward(self,ex,encoder_hss):\n",
    "        ex = input, output\n",
    "        \n",
    "        output_length = len(output)\n",
    "\n",
    "        embedded_output = self.embedding(output[:-1])\n",
    "        #print(embedded_output)\n",
    "        results = []\n",
    "        decoder_state = (torch.zeros(1,1,RNN_HIDDEN_DIM,requires_grad=False), \n",
    "                         torch.zeros(1,1,RNN_HIDDEN_DIM,requires_grad=False))\n",
    "        \n",
    "        #decoder_states_expanded = decoder_state[0].expand(embedded_output.shape[0],-1,-1)\n",
    "        for i in range(output_length-1):\n",
    "            context = self.attention(encoder_hss, decoder_state[0])\n",
    "            _, decoder_state = self.rnn(torch.cat([embedded_output[i].unsqueeze(0), context], dim=2), decoder_state)\n",
    "            decoder_hs = decoder_state[0]\n",
    "            #print(decoder_state[0].shape)\n",
    "            result = self.hidden2char(decoder_hs)\n",
    "            #print(log_softmax(result, dim=2).shape)\n",
    "            results.append(result)\n",
    "        \n",
    "        # print(\"results\")\n",
    "        # print(torch.cat(results).shape)\n",
    "        # print(log_softmax(torch.cat(results), dim=2))\n",
    "        return log_softmax(torch.cat(results), dim=2), output[1:]\n",
    "                    \n",
    "    def generate(self,encoder_hss):\n",
    "        with torch.no_grad():\n",
    "            decoder_state = (torch.zeros(1,1,RNN_HIDDEN_DIM), torch.zeros(1,1,RNN_HIDDEN_DIM))\n",
    "            output_char = torch.LongTensor([[self.alphabet[\"<start>\"]]])\n",
    "            result = []\n",
    "            for _ in range(MAXWFLEN):\n",
    "                output_embedding = self.embedding(output_char)\n",
    "                context = self.attention(encoder_hss, decoder_state[0])\n",
    "                #print(context.shape)\n",
    "                _, decoder_state = self.rnn(torch.cat([output_embedding, context], dim=2), decoder_state)\n",
    "                output_char =  torch.LongTensor([[self.hidden2char(decoder_state[0]).argmax()]])\n",
    "                #print(output_char.numpy().tolist()[0][0])\n",
    "\n",
    "                result.append(output_char.numpy().tolist()[0][0])\n",
    "            return result\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "122bca91-5cdc-4b50-b922-1994b15a23e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention,self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(3*RNN_HIDDEN_DIM,RNN_HIDDEN_DIM)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.linear2 = nn.Linear(RNN_HIDDEN_DIM,1)\n",
    "    \n",
    "    def forward(self,encoder_hss,decoder_hs):\n",
    "        # your code here\n",
    "        decoder_hs = decoder_hs.expand(encoder_hss.size()[0],-1,-1)\n",
    "        conditioned = torch.cat([decoder_hs,encoder_hss],dim=2)\n",
    "        h1 = self.relu(self.linear1(conditioned))\n",
    "        h2 = self.linear2(h1).softmax(dim=0)\n",
    "        weights = h2.expand(-1,-1,2*RNN_HIDDEN_DIM)\n",
    "        weighted_mean = torch.sum(weights * encoder_hss, dim=0)\n",
    "        return weighted_mean.unsqueeze(0)\n",
    "        # your code here\n",
    "\n",
    "# # An assertion to test that your implementation returns an object of the correct size. \n",
    "# input, input_length = example.input\n",
    "# encoder_hss = Encoder(vocab.get_stoi())(example)\n",
    "# decoder_hs = torch.randn(1,1,RNN_HIDDEN_DIM)\n",
    "\n",
    "# assert(Attention()(encoder_hss,decoder_hs).size() == torch.Size([1,1,2*RNN_HIDDEN_DIM]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "e388a690-98af-4dde-af0b-be3059ac00bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 64619 of 111771\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [122], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_function(tag_scores,tgt) \n\u001b[1;32m     49\u001b[0m     tot_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mdetach()\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m---> 50\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[1;32m     51\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m()\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.10/site-packages/torch/_tensor.py:488\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    480\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    481\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    486\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    487\u001b[0m     )\n\u001b[0;32m--> 488\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    489\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.10/site-packages/torch/autograd/__init__.py:197\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    192\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    194\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 197\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class WordInflector(nn.Module):\n",
    "    def __init__(self, alphabet):\n",
    "        super(WordInflector, self).__init__()\n",
    "        self.c2i = alphabet.get_stoi()\n",
    "        self.i2c = alphabet.get_itos()\n",
    "        alphabet_size = len(self.c2i)\n",
    "        \n",
    "        self.encoder = Encoder(self.c2i)\n",
    "        self.decoder = Decoder(self.c2i)\n",
    "    \n",
    "    def get_string(self,ids):\n",
    "        string = ''.join([self.i2c[i] for i in ids])\n",
    "        return re.sub(\"%s.*\" % \"<end>\",\"\",string)\n",
    "\n",
    "    def forward(self, example):\n",
    "        encoder_hs = self.encoder(example)\n",
    "        return self.decoder(example,encoder_hs)\n",
    "            \n",
    "    def generate(self, data):\n",
    "        all_results = []\n",
    "        with torch.no_grad():\n",
    "            for example in data:\n",
    "                encoder_hs = self.encoder(example)\n",
    "                output = self.decoder.generate(encoder_hs)\n",
    "                all_results.append(self.get_string(output))\n",
    "        return all_results\n",
    "    \n",
    "if __name__==\"__main__\":\n",
    "    train_iter, dev_iter = train_dataloader, dev_dataloader\n",
    "    vocab = voc\n",
    "    \n",
    "    inflector = WordInflector(vocab)\n",
    "\n",
    "    loss_function = nn.NLLLoss(ignore_index=inflector.c2i[\"<pad>\"],reduction='mean')\n",
    "    optimizer = Adam(inflector.parameters())\n",
    "    gold_dev_words = [''.join(output) for input,output in dev_iter.dataset]\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        tot_loss = 0 \n",
    "\n",
    "        # Update parameters\n",
    "        for i, batch in enumerate(train_iter):\n",
    "            print(\"Example %u of %u\" % (i+1,len(train_iter)),end=\"\\r\")\n",
    "            inflector.zero_grad()\n",
    "            tag_scores, tgt = inflector(batch)\n",
    "            tgt = tgt.permute(1,0)\n",
    "            tag_scores = tag_scores.permute(1,2,0)\n",
    "            loss = loss_function(tag_scores,tgt) \n",
    "            tot_loss += loss.detach().numpy()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print()\n",
    "        avg_loss = tot_loss/len(train_iter)\n",
    "        print(\"EPOCH %u: AVG LOSS PER EX: %.5f\" % (epoch+1,avg_loss))        \n",
    "\n",
    "        # Evaluate on dev data.\n",
    "        sys_dev_words = inflector.generate(dev_iter)\n",
    "        print(\"DEV ACC: %.2f%%\" % accuracy(sys_dev_words,gold_dev_words))\n",
    "        \n",
    "        torch.save(inflector , \"ckpt/\"+\"tsez-\" + str(epoch) + \".pt\")\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb730b4-3116-4a72-b081-4859d002f6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys_dev_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ca4b648-a941-4bc3-909e-dc1f4013174b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
